{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controlling gpu usage for NLI model\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import json\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials, firestore\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_firebase():\n",
    "    # cred = credentials.Certificate(\"YOUR/PATH/TO/firebase-adminsdk-XXXXXX-XXXXXXXXXX.json\") # Call once on creation\n",
    "    service_account_info = {\n",
    "        \"type\": st.secrets[\"firebase\"][\"type\"],\n",
    "        \"project_id\": st.secrets[\"firebase\"][\"project_id\"],\n",
    "        \"private_key_id\": st.secrets[\"firebase\"][\"private_key_id\"],\n",
    "        \"private_key\": st.secrets[\"firebase\"][\"private_key\"].replace(\"\\\\n\", \"\\n\"),\n",
    "        \"client_email\": st.secrets[\"firebase\"][\"client_email\"],\n",
    "        \"client_id\": st.secrets[\"firebase\"][\"client_id\"],\n",
    "        \"auth_uri\": st.secrets[\"firebase\"][\"auth_uri\"],\n",
    "        \"token_uri\": st.secrets[\"firebase\"][\"token_uri\"],\n",
    "        \"auth_provider_x509_cert_url\": st.secrets[\"firebase\"][\"auth_provider_x509_cert_url\"],\n",
    "        \"client_x509_cert_url\": st.secrets[\"firebase\"][\"client_x509_cert_url\"]\n",
    "    }\n",
    "    if not firebase_admin._apps:\n",
    "        cred = credentials.Certificate(service_account_info)\n",
    "        firebase_admin.initialize_app(cred)\n",
    "    return firestore.client()\n",
    "\n",
    "# Initialize Firestore\n",
    "db = initialize_firebase()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_collections():\n",
    "    \"\"\"\n",
    "    Retrieve all collection names from the Firestore database.\n",
    "    \"\"\"\n",
    "    collections = db.collections()\n",
    "    return [collection.id for collection in collections]\n",
    "\n",
    "\n",
    "def get_all_documents_from_collection(collection_name):\n",
    "    \"\"\"\n",
    "    Retrieve all documents from a specified Firestore collection.\n",
    "\n",
    "    Args:\n",
    "        collection_name (str): The name of the Firestore collection.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries representing the documents in the collection.\n",
    "    \"\"\"\n",
    "    collection_ref = db.collection(collection_name)\n",
    "    docs = collection_ref.stream()\n",
    "\n",
    "    all_documents = []\n",
    "    for doc in docs:\n",
    "        document_data = doc.to_dict()\n",
    "        document_data['id'] = doc.id  # Optionally include the document ID\n",
    "        all_documents.append(document_data)\n",
    "    \n",
    "    return all_documents\n",
    "\n",
    "\n",
    "# Fetch all collections and documents\n",
    "collections = get_all_collections()\n",
    "all_data = []\n",
    "\n",
    "for collection in collections:\n",
    "    print(f\"Processing collection: {collection}\")\n",
    "    documents = get_all_documents_from_collection(collection)\n",
    "    all_data.extend(documents)\n",
    "\n",
    "display(f'{len(all_data)=}')\n",
    "display(f'{all_data[0]=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply some filtering criteria on your data if needed\n",
    "filtered_data = []\n",
    "\n",
    "for sample in all_data:\n",
    "    filtered_data.append(sample)\n",
    "\n",
    "json.dump(filtered_data, open(\"0_human_eval_mix_labelled.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the questions, docs and responses with ratings\n",
    "\n",
    "labelled = json.load(open(\"0_human_eval_mix_labelled.json\")) # from firebase\n",
    "unlabelled = json.load(open(\"0_human_eval_mix_unlabelled.json\")) # json wth your questions, docs and responses (you must have this prior to starting this)\n",
    "\n",
    "for label_sample in labelled:\n",
    "    found = False  # Flag to indicate if a match was found\n",
    "    for unlabel_sample in unlabelled:\n",
    "        if label_sample['question'] == unlabel_sample['question']:\n",
    "            label_sample['pos'] = unlabel_sample['GAns']\n",
    "            label_sample['neg'] = unlabel_sample['output']\n",
    "            label_sample['docs'] = unlabel_sample['docs']\n",
    "            found = True\n",
    "            break  # Exit the loop if a match is found\n",
    "    if not found:\n",
    "        print(\"missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the dataset labels from raw dataset\n",
    "\n",
    "# Load raw data from different datasets\n",
    "asqa_raw_data = json.load(open(\"asqa_error_instruction.json\"))\n",
    "eli5_raw_data = json.load(open(\"eli5_error_instruction.json\"))\n",
    "qampari_raw_data = json.load(open(\"qampari_error_instruction.json\"))\n",
    "\n",
    "# Add dataset labels to each sample for identification\n",
    "for sample in asqa_raw_data:\n",
    "    sample[\"dataset\"] = \"ASQA\"\n",
    "for sample in eli5_raw_data:\n",
    "    sample[\"dataset\"] = \"ELI5\"\n",
    "for sample in qampari_raw_data:\n",
    "    sample[\"dataset\"] = \"QAMPARI\"\n",
    "\n",
    "# Combine all raw data into a single list\n",
    "combined_raw_data = asqa_raw_data + eli5_raw_data + qampari_raw_data\n",
    "\n",
    "missing = 0 \n",
    "missing_samples = []\n",
    "final_data = []\n",
    "\n",
    "# Function to check if a labelled sample matches a raw sample\n",
    "def match_sample(label_sample, unlabel_sample):\n",
    "    return label_sample['question'] == unlabel_sample['question'] \n",
    "\n",
    "for label_sample in labelled:\n",
    "    found = False  # Flag to indicate if a match was found\n",
    "    for raw_sample in combined_raw_data:\n",
    "        if match_sample(label_sample, raw_sample):\n",
    "            # label_sample['eval_metrics'] = raw_sample['eval_metrics']\n",
    "            label_sample['dataset'] = raw_sample['dataset']  # Add dataset label\n",
    "            final_data.append(label_sample)\n",
    "            found = True\n",
    "            break  # Exit the loop if a match is found\n",
    "    if not found:\n",
    "        missing +=1\n",
    "        missing_samples.append(label_sample)\n",
    "\n",
    "print(f'{missing=}')\n",
    "print(f'{len(final_data)=}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json.dump(updated_final_data, open(\"human_eval_mix_labelled_complete.json\", \"w\"), indent=4)\n",
    "updated_final_data = json.load(open(\"human_eval_mix_labelled_complete.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "# get NLI rating\n",
    "\n",
    "# Run AutoAIS evaluation for \"pos\" and \"neg\" responses\n",
    "updated_final_data = compute_autoais(updated_final_data, \"pos\")\n",
    "updated_final_data = compute_autoais(updated_final_data, \"neg\")\n",
    "\n",
    "json.dump(updated_final_data, open(\"1_human_eval_mix_labelled_complete.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder and clean up data\n",
    "reordered_data = []\n",
    "\n",
    "for sample in updated_final_data:\n",
    "    # Add correctness values to the NLI scores\n",
    "    sample['nli']['pos']['correctness'] = 1\n",
    "    sample['nli']['neg']['correctness'] = 0\n",
    "\n",
    "    # Reorder keys and structure the sample\n",
    "    reordered_sample = {\n",
    "        'question_set': sample['question_set'],\n",
    "        'question': sample['question'],\n",
    "        'docs': sample['docs'],\n",
    "        'pos': sample['pos'],\n",
    "        'neg': sample['neg'],\n",
    "        'human': {\n",
    "            'pos': sample['response']['pos'],\n",
    "            'neg': sample['response']['neg']\n",
    "        },\n",
    "        'nli': sample['nli'],\n",
    "        'dataset': sample['dataset'],\n",
    "        'id': sample['id']\n",
    "    }\n",
    "    reordered_data.append(reordered_sample)\n",
    "\n",
    "json.dump(reordered_data, open(\"2_human_eval_mix_labelled_cleaned.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to binarize the human ratings\n",
    "def binarize_nli(data):\n",
    "    # Mapping of text-based labels to binary values\n",
    "    binarize_map = {\n",
    "        \"Full support\": 1,\n",
    "        \"Partial support\": 1,\n",
    "        \"No support\": 0,\n",
    "        \"Correct\": 1,\n",
    "        \"Wrong\": 0\n",
    "    }\n",
    "\n",
    "    for item in data:\n",
    "        # Ensure the 'human' field exists in the item\n",
    "        if 'human' in item:\n",
    "            # Process both 'pos' and 'neg' fields in 'human'\n",
    "            for response_type in ['pos', 'neg']:\n",
    "                if response_type in item['human']:\n",
    "                    # Convert each key-value pair in the field\n",
    "                    for key, value in item['human'][response_type].items():\n",
    "                        item['human'][response_type][key] = binarize_map.get(value, value)\n",
    "\n",
    "    return data\n",
    "\n",
    "binarized_data = binarize_nli(reordered_data)\n",
    "json.dump(binarized_data, open(\"3_human_eval_mix_labelled_binarized.json\", \"w\"), indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_samples(data):\n",
    "    # Initialize counters\n",
    "    correct_positive_samples = 0\n",
    "    correct_negative_samples = 0\n",
    "    correct_citation_ratings = 0\n",
    "    total_citation_ratings = 0\n",
    "    correct_citation_prec = 0\n",
    "    total_citation_prec = 0\n",
    "    correct_citation_rec = 0\n",
    "    total_citation_rec = 0\n",
    "\n",
    "    for sample in data:\n",
    "        # Check if positive and negative samples are correctly rated\n",
    "        if sample['human']['pos']['correctness'] == sample['nli']['pos']['correctness'] == 1:\n",
    "            correct_positive_samples += 1\n",
    "        if sample['human']['neg']['correctness'] == sample['nli']['neg']['correctness'] == 0:\n",
    "            correct_negative_samples += 1\n",
    "\n",
    "        # Calculate citation ratings accuracy\n",
    "        for response_type in ['pos', 'neg']:\n",
    "            for key, human_rating in sample['human'][response_type].items():\n",
    "                # Exclude \"correctness\" as itâ€™s not a citation\n",
    "                if key != \"correctness\" and key in sample['nli'][response_type]:\n",
    "                    nli_rating = sample['nli'][response_type][key]\n",
    "                    \n",
    "                    # Overall citation accuracy\n",
    "                    if human_rating == nli_rating:\n",
    "                        correct_citation_ratings += 1\n",
    "                    total_citation_ratings += 1\n",
    "\n",
    "                    # Separate into precision and recall metrics\n",
    "                    if \"prec\" in key:\n",
    "                        total_citation_prec += 1\n",
    "                        if human_rating == nli_rating:\n",
    "                            correct_citation_prec += 1\n",
    "                    elif \"recall\" in key:\n",
    "                        total_citation_rec += 1\n",
    "                        if human_rating == nli_rating:\n",
    "                            correct_citation_rec += 1\n",
    "\n",
    "    # Output results\n",
    "    print(f\"Correct samples: {correct_positive_samples + correct_negative_samples} out of {len(data) * 2} ({(correct_positive_samples + correct_negative_samples) / (len(data) * 2):.2%})\")\n",
    "    print(f\"Correct positive samples: {correct_positive_samples} out of {len(data)} ({correct_positive_samples / len(data):.2%})\")\n",
    "    print(f\"Correct negative samples: {correct_negative_samples} out of {len(data)} ({correct_negative_samples / len(data):.2%})\")\n",
    "    citation_acc = (correct_citation_ratings / total_citation_ratings) if total_citation_ratings > 0 else \"No citation ratings available\"\n",
    "    print(f\"Correct citation ratings: {correct_citation_ratings} out of {total_citation_ratings} ({citation_acc:.2%})\")\n",
    "    print(f\"Correct citation precision ratings: {correct_citation_prec} out of {total_citation_prec} ({correct_citation_prec / total_citation_prec:.2%})\" if total_citation_prec > 0 else \"No citation precision ratings available\")\n",
    "    print(f\"Correct citation recall ratings: {correct_citation_rec} out of {total_citation_rec} ({correct_citation_rec / total_citation_rec:.2%})\" if total_citation_rec > 0 else \"No citation recall ratings available\")\n",
    "\n",
    "# Example usage with data\n",
    "analyze_samples(final_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cohen_kappa(data):\n",
    "    # Initialize counters for agreement and disagreement\n",
    "    # Contingency table values\n",
    "    agree_correct = 0  # Both human and model say \"Correct\"\n",
    "    agree_wrong = 0    # Both human and model say \"Wrong\"\n",
    "    human_correct_model_wrong = 0  # Human says \"Correct\", model says \"Wrong\"\n",
    "    human_wrong_model_correct = 0  # Human says \"Wrong\", model says \"Correct\"\n",
    "\n",
    "    for sample in data:\n",
    "        # Check 'pos' ratings\n",
    "        human_pos = sample['human']['pos']['correctness']\n",
    "        model_pos = sample['nli']['pos']['correctness']\n",
    "        if human_pos == 1 and model_pos == 1:\n",
    "            agree_correct += 1\n",
    "        elif human_pos == 0 and model_pos == 0:\n",
    "            agree_wrong += 1\n",
    "        elif human_pos == 1 and model_pos == 0:\n",
    "            human_correct_model_wrong += 1\n",
    "        elif human_pos == 0 and model_pos == 1:\n",
    "            human_wrong_model_correct += 1\n",
    "\n",
    "        # Check 'neg' ratings\n",
    "        human_neg = sample['human']['neg']['correctness']\n",
    "        model_neg = sample['nli']['neg']['correctness']\n",
    "        if human_neg == 1 and model_neg == 1:\n",
    "            agree_correct += 1\n",
    "        elif human_neg == 0 and model_neg == 0:\n",
    "            agree_wrong += 1\n",
    "        elif human_neg == 1 and model_neg == 0:\n",
    "            human_correct_model_wrong += 1\n",
    "        elif human_neg == 0 and model_neg == 1:\n",
    "            human_wrong_model_correct += 1\n",
    "\n",
    "    # Total number of ratings\n",
    "    total = agree_correct + agree_wrong + human_correct_model_wrong + human_wrong_model_correct\n",
    "\n",
    "    # Observed agreement\n",
    "    observed_agreement = (agree_correct + agree_wrong) / total\n",
    "\n",
    "    # Expected agreement by chance\n",
    "    prob_human_correct = (agree_correct + human_correct_model_wrong) / total\n",
    "    prob_model_correct = (agree_correct + human_wrong_model_correct) / total\n",
    "    prob_human_wrong = (agree_wrong + human_wrong_model_correct) / total\n",
    "    prob_model_wrong = (agree_wrong + human_correct_model_wrong) / total\n",
    "\n",
    "    expected_agreement = (prob_human_correct * prob_model_correct) + (prob_human_wrong * prob_model_wrong)\n",
    "\n",
    "    # Cohen's Kappa\n",
    "    kappa = (observed_agreement - expected_agreement) / (1 - expected_agreement) if (1 - expected_agreement) != 0 else 0\n",
    "\n",
    "    # Display results\n",
    "    print(\"Cohen's Kappa:\", kappa)\n",
    "\n",
    "# Example usage with data\n",
    "compute_cohen_kappa(final_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cohen_kappa_citations(data):\n",
    "    # Initialize counters for agreement and disagreement\n",
    "    # Contingency table values for citation judgments\n",
    "    agree_correct = 0  # Both human and model say \"Correct\" (1)\n",
    "    agree_wrong = 0    # Both human and model say \"Wrong\" (0)\n",
    "    human_correct_model_wrong = 0  # Human says \"Correct\" (1), model says \"Wrong\" (0)\n",
    "    human_wrong_model_correct = 0  # Human says \"Wrong\" (0), model says \"Correct\" (1)\n",
    "\n",
    "    # Iterate over each sample\n",
    "    for sample in data:\n",
    "        for response_type in ['pos', 'neg']:\n",
    "            # Ensure there are citations in both human and model (nli) judgments\n",
    "            human_judgments = sample['human'][response_type]\n",
    "            model_judgments = sample['nli'][response_type]\n",
    "            \n",
    "            # Loop through citation fields only (excluding \"correctness\")\n",
    "            for key in human_judgments:\n",
    "                if key != \"correctness\" and key in model_judgments:\n",
    "                    human_value = human_judgments[key]\n",
    "                    model_value = model_judgments[key]\n",
    "\n",
    "                    # Update counters based on agreement/disagreement\n",
    "                    if human_value == 1 and model_value == 1:\n",
    "                        agree_correct += 1\n",
    "                    elif human_value == 0 and model_value == 0:\n",
    "                        agree_wrong += 1\n",
    "                    elif human_value == 1 and model_value == 0:\n",
    "                        human_correct_model_wrong += 1\n",
    "                    elif human_value == 0 and model_value == 1:\n",
    "                        human_wrong_model_correct += 1\n",
    "\n",
    "    # Total number of citation judgments\n",
    "    total = agree_correct + agree_wrong + human_correct_model_wrong + human_wrong_model_correct\n",
    "\n",
    "    # Observed agreement\n",
    "    observed_agreement = (agree_correct + agree_wrong) / total if total > 0 else 0\n",
    "\n",
    "    # Expected agreement by chance\n",
    "    prob_human_correct = (agree_correct + human_correct_model_wrong) / total if total > 0 else 0\n",
    "    prob_model_correct = (agree_correct + human_wrong_model_correct) / total if total > 0 else 0\n",
    "    prob_human_wrong = (agree_wrong + human_wrong_model_correct) / total if total > 0 else 0\n",
    "    prob_model_wrong = (agree_wrong + human_correct_model_wrong) / total if total > 0 else 0\n",
    "\n",
    "    expected_agreement = (prob_human_correct * prob_model_correct) + (prob_human_wrong * prob_model_wrong)\n",
    "\n",
    "    # Cohen's Kappa\n",
    "    kappa = (observed_agreement - expected_agreement) / (1 - expected_agreement) if (1 - expected_agreement) != 0 else 0\n",
    "\n",
    "    # Display results\n",
    "    print(\"Cohen's Kappa for Citation Judgments:\", kappa)\n",
    "\n",
    "# Example usage with binarized data\n",
    "compute_cohen_kappa_citations(final_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
